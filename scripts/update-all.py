#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Scoop Bucket Update Orchestrator
Runs all update scripts for the scoop-alts bucket and provides a summary report.
"""

import subprocess
import sys
import time
from pathlib import Path
from datetime import datetime
import argparse
import concurrent.futures
import os
import json
from typing import List, Tuple, Dict
import threading
import logging
import requests

# Set UTF-8 encoding for Windows console
if sys.platform == "win32":
    try:
        import codecs
        if hasattr(sys.stdout, "detach"):
            sys.stdout = codecs.getwriter("utf-8")(sys.stdout.detach())
        if hasattr(sys.stderr, "detach"):
            sys.stderr = codecs.getwriter("utf-8")(sys.stderr.detach())
    except Exception:
        os.environ.setdefault("PYTHONIOENCODING", "utf-8")

# Configuration
SCRIPTS_DIR = Path(__file__).parent
REPO_ROOT = SCRIPTS_DIR.parent

# Performance and Concurrency Constants
DEFAULT_TIMEOUT = int(os.environ.get('SCOOP_UPDATE_TIMEOUT', '120'))  # seconds per script execution
DEFAULT_WORKERS = int(os.environ.get('SCOOP_UPDATE_WORKERS', '6'))    # default parallel workers
DEFAULT_RETRY_ATTEMPTS = int(os.environ.get('SCOOP_RETRY_ATTEMPTS', '0'))  # no retries by default

# Provider-specific rate limiting
MAX_GITHUB_WORKERS = int(os.environ.get('MAX_GITHUB_WORKERS', '3'))      # GitHub API rate limit consideration
MAX_MICROSOFT_WORKERS = int(os.environ.get('MAX_MICROSOFT_WORKERS', '3'))   # Microsoft servers rate limit
MAX_GOOGLE_WORKERS = int(os.environ.get('MAX_GOOGLE_WORKERS', '4'))      # Google APIs rate limit

# Provider-specific configuration
PROVIDER_CONFIGS = {
    'github': {
        'max_workers': MAX_GITHUB_WORKERS,
        'base_url': 'https://api.github.com',
        'rate_limit_buffer': 100  # requests per hour buffer
    },
    'microsoft': {
        'max_workers': MAX_MICROSOFT_WORKERS,
        'base_url': 'https://www.microsoft.com',
        'retry_delay': 2.0  # seconds between retries
    },
    'google': {
        'max_workers': MAX_GOOGLE_WORKERS,
        'base_url': 'https://www.googleapis.com',
        'timeout_multiplier': 1.5  # longer timeouts for Google APIs
    }
}

# Logging configuration
LOG_LEVEL = os.environ.get('LOG_LEVEL', 'INFO').upper()
LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
LOG_DATE_FORMAT = '%H:%M:%S'

# File and directory constants
BUCKET_DIR = REPO_ROOT / 'bucket'
SCRIPTS_GLOB = 'update-*.py'
MANIFEST_EXTENSION = '.json'
MAX_MANIFEST_SIZE = 10 * 1024 * 1024  # 10MB max manifest file size

# Cache configuration
CACHE_EXPIRY_SECONDS = int(os.environ.get('CACHE_EXPIRY_SECONDS', '1800'))  # 30 minutes cache TTL

# Cache manifest versions in-memory during one orchestrator run to avoid
# repeated disk reads when printing summaries and composing commit messages.
MANIFEST_VERSION_CACHE: Dict[str, str] = {}

PREFER_STRUCTURED_OUTPUT = False

def run_git_command(args: List[str], cwd: Path = REPO_ROOT) -> Tuple[int, str, str]:
    """Run a git command and return (returncode, stdout, stderr)."""
    try:
        result = subprocess.run(
            args,
            capture_output=True,
            text=True,
            cwd=str(cwd),
            encoding="utf-8",
            errors="replace"  # Handle encoding errors gracefully
        )
        return result.returncode, result.stdout.strip(), result.stderr.strip()
    except Exception as e:
        logging.error(f"Git command failed: {e}")
        return 1, "", str(e)

def is_file_tracked(path: Path) -> bool:
    """Return True if the file is tracked by git."""
    rc, _, _ = run_git_command(["git", "ls-files", "--error-unmatch", str(path)])
    return rc == 0

def stage_bucket_changes() -> None:
    """Stage changes inside the bucket directory."""
    rc, out, err = run_git_command(["git", "add", "bucket"])
    if rc != 0:
        print(f"‚ö†Ô∏è  git add failed: {err or out}")

def get_staged_bucket_changes() -> Tuple[List[str], List[str]]:
    """Return (added_apps, updated_apps) from staged changes under bucket/."""
    rc, out, err = run_git_command(["git", "diff", "--cached", "--name-status"])
    if rc != 0:
        print(f"‚ö†Ô∏è  git diff --cached failed: {err or out}")
        return [], []

    added_apps: List[str] = []
    updated_apps: List[str] = []

    for line in out.splitlines():
        if not line.strip():
            continue
        parts = line.split("\t", 1)
        if len(parts) != 2:
            continue
        status, path = parts
        # Only consider manifests in bucket/
        if not path.startswith(f"{BUCKET_DIR.name}/") or not path.endswith(MANIFEST_EXTENSION):
            continue
        app_name = Path(path).stem
        if status.startswith("A"):  # Added
            added_apps.append(app_name)
        elif status.startswith("M") or status.startswith("R"):  # Modified or Renamed
            updated_apps.append(app_name)

    added_apps.sort()
    updated_apps.sort()
    return added_apps, updated_apps

def commit_with_message(message: str) -> bool:
    """Create a commit with the given message. Returns True if commit succeeded."""
    rc, out, err = run_git_command(["git", "commit", "-m", message])
    if rc != 0:
        # If there is nothing to commit, git returns non-zero; report and skip
        reason = err or out
        if "nothing to commit" in (reason.lower()):
            print("‚ÑπÔ∏è  No changes staged to commit.")
        else:
            print(f"‚ö†Ô∏è  git commit failed: {reason}")
        return False
    print(out or "‚úÖ Commit created")
    return True

def push_changes() -> None:
    """Push committed changes to the remote (env overrides supported)."""
    remote = os.environ.get("SCOOP_GIT_REMOTE", "origin")
    branch = os.environ.get("SCOOP_GIT_BRANCH")
    if os.environ.get("SCOOP_GIT_DRY_RUN") == "1":
        print(f"‚ÑπÔ∏è  Dry-run: would push to {remote}{' ' + branch if branch else ''}")
        return
    args = ["git", "push", remote] + ([branch] if branch else [])
    rc, out, err = run_git_command(args)
    if rc != 0:
        print(f"‚ö†Ô∏è  git push failed: {err or out}")
    else:
        print(out or "‚¨ÜÔ∏è  Pushed changes to remote")

def get_manifest_version(app_name: str) -> str:
    """Return version string from bucket/<app_name>.json if available, using cache."""
    # Return cached value if present
    if app_name in MANIFEST_VERSION_CACHE:
        return MANIFEST_VERSION_CACHE.get(app_name, "")

    manifest_path = BUCKET_DIR / f"{app_name}{MANIFEST_EXTENSION}"
    try:
        # Check file size before reading
        if manifest_path.exists() and manifest_path.stat().st_size > MAX_MANIFEST_SIZE:
            logging.warning(f"Manifest file too large: {manifest_path} ({manifest_path.stat().st_size} bytes)")
            MANIFEST_VERSION_CACHE[app_name] = ""
            return ""

        with open(manifest_path, "r", encoding="utf-8") as f:
            manifest = json.load(f)
            version = str(manifest.get("version", "")).strip()
            MANIFEST_VERSION_CACHE[app_name] = version
            return version
    except Exception as e:
        logging.debug(f"Failed to read manifest {manifest_path}: {e}")
        # Cache miss with empty value to avoid reattempting in this run
        MANIFEST_VERSION_CACHE[app_name] = ""
        return ""

def list_untracked_manifests() -> List[Tuple[str, Path]]:
    """Return a list of (app_name, path) for untracked manifests under bucket/."""
    rc, out, err = run_git_command(["git", "ls-files", "--others", "--exclude-standard", str(BUCKET_DIR)])
    if rc != 0:
        print(f"‚ö†Ô∏è  git ls-files failed: {err or out}")
        return []
    manifests: List[Tuple[str, Path]] = []
    for rel in out.splitlines():
        rel = rel.strip()
        if not rel:
            continue
        if not rel.endswith(MANIFEST_EXTENSION):
            continue
        if not rel.startswith(f"{BUCKET_DIR.name}/"):
            continue
        p = REPO_ROOT / rel
        app_name = Path(rel).stem
        manifests.append((app_name, p))
    return manifests

def stage_and_commit_per_package(updated_results: List["UpdateResult"]) -> None:
    """Stage and commit changes per updated package manifest under bucket/."""
    for r in updated_results:
        # Derive package name from script name: update-<pkg>.py
        pkg = r.script_name.replace('update-', '').replace('.py', '')
        manifest_path = BUCKET_DIR / f"{pkg}{MANIFEST_EXTENSION}"

        if not manifest_path.exists():
            # If the script updated something else or the manifest name differs, skip gracefully
            continue

        # Stage only this manifest
        rc, out, err = run_git_command(["git", "add", str(manifest_path)])
        if rc != 0:
            print(f"‚ö†Ô∏è  git add {manifest_path} failed: {err or out}")
            continue

        # Determine if there's a staged change and whether it's new or modified
        rc, ns_out, ns_err = run_git_command(["git", "diff", "--cached", "--name-status", "--", str(manifest_path)])
        if not ns_out.strip():
            print(f"‚ÑπÔ∏è  No staged changes for {pkg}, skipping commit.")
            continue
        status_line = ns_out.strip().splitlines()[0]
        status_code = status_line.split("\t", 1)[0] if "\t" in status_line else ""
        new_file = status_code.startswith("A")

        # Read manifest version, if available
        version_str = get_manifest_version(pkg)

        if new_file:
            msg = f"{pkg}: Add version {version_str} (script: {r.script_name})" if version_str else f"{pkg}: Add manifest (script: {r.script_name})"
        else:
            msg = f"{pkg}: Update to version {version_str} (script: {r.script_name})" if version_str else f"{pkg}: Update manifest (script: {r.script_name})"
        commit_with_message(msg)

def discover_update_scripts() -> List[str]:
    """Automatically discover all update-*.py scripts in the scripts directory"""
    update_scripts = []

    # Find all update-*.py files
    for script_file in SCRIPTS_DIR.glob(SCRIPTS_GLOB):
        # Skip the update-all.py script itself and utility scripts
        if script_file.name not in ["update-all.py", "update-script-generator.py"] and not script_file.name.startswith("_"):
            update_scripts.append(script_file.name)

    # Sort for consistent ordering
    update_scripts.sort()

    print(f"üîç Discovered {len(update_scripts)} update scripts:")
    for script in update_scripts:
        print(f"   ‚Ä¢ {script}")

    return update_scripts

class UpdateResult:
    """Class to store update results."""
    def __init__(self, script_name: str, success: bool, output: str, duration: float, updated: bool = False):
        self.script_name = script_name
        self.success = success
        self.output = output
        self.duration = duration
        self.updated = updated

def parse_script_output(output: str, script_name: str) -> tuple[bool, bool]:
    """Parse script output to determine update status."""
    # Prefer structured JSON result: search the last up to 10 non-empty lines for a JSON object
    structured_updated = None
    try:
        lines = [ln for ln in output.strip().splitlines() if ln.strip()]
        for ln in reversed(lines[-10:]):
            last = ln.strip()
            if last.startswith('{') and last.endswith('}'):
                import json as _json
                parsed = _json.loads(last)
                if isinstance(parsed, dict) and 'updated' in parsed:
                    structured_updated = bool(parsed.get('updated'))
                    # Prime version cache if provided
                    pkg = script_name.replace('update-', '').replace('.py', '')
                    v = parsed.get('version')
                    if isinstance(v, str) and v:
                        MANIFEST_VERSION_CACHE[pkg] = v
                    break
    except Exception:
        # Ignore JSON parsing issues; fall back to text heuristics
        pass

    if structured_updated is not None:
        updated = structured_updated
        no_update_needed = not updated
    else:
        if PREFER_STRUCTURED_OUTPUT:
            updated = False
            no_update_needed = False
        else:
            lower = output.lower()
            updated = ("update completed successfully" in lower) or ("updated" in lower)
            no_update_needed = ("no update needed" in lower) or ("up to date" in lower)

    return updated, no_update_needed

def run_update_script(script_path: Path, timeout: int = 300) -> UpdateResult:
    """Run a single update script and return the result."""
    script_name = script_path.name
    start_time = time.time()

    try:
        logging.info(f"Running {script_name}...")
        print(f"üöÄ Running {script_name}...")

        # Run the script
        # Change to the parent directory (where bucket/ is located) before running the script
        parent_dir = SCRIPTS_DIR.parent

        # Set environment to handle Unicode properly
        env = os.environ.copy()
        env['PYTHONIOENCODING'] = 'utf-8'

        result = subprocess.run(
            [sys.executable, str(script_path)],
            capture_output=True,
            text=True,
            timeout=timeout,
            cwd=parent_dir,
            encoding='utf-8',
            errors='replace',
            env=env
        )

        duration = time.time() - start_time

        # Check if update was successful and if anything was updated
        output = result.stdout + result.stderr

        # Parse output for update status
        updated, no_update_needed = parse_script_output(output, script_name)

        if result.returncode == 0:
            if updated:
                print(f"‚úÖ {script_name} - Updated successfully ({duration:.1f}s)")
                logging.info(f"{script_name} updated successfully")
            elif no_update_needed:
                print(f"‚ÑπÔ∏è  {script_name} - No update needed ({duration:.1f}s)")
                logging.info(f"{script_name} no update needed")
            else:
                print(f"‚úÖ {script_name} - Completed ({duration:.1f}s)")
                logging.info(f"{script_name} completed without updates")

            return UpdateResult(script_name, True, output, duration, updated)
        else:
            error_msg = result.stderr.strip() if result.stderr else 'Unknown error'
            stdout_msg = result.stdout.strip() if result.stdout else ''
            detailed_error = f"Exit code: {result.returncode}\nSTDERR: {error_msg}\nSTDOUT: {stdout_msg}"
            logging.error(f"{script_name} failed: {detailed_error}")
            print(f"‚ùå {script_name} - Failed ({duration:.1f}s)")
            print(f"   Error details: {detailed_error}")
            return UpdateResult(script_name, False, detailed_error, duration, False)

    except subprocess.TimeoutExpired:
        duration = time.time() - start_time
        logging.error(f"{script_name} timed out after {timeout}s")
        print(f"‚è∞ {script_name} - Timeout after {timeout}s")
        return UpdateResult(script_name, False, f"Script timed out after {timeout} seconds", duration, False)

    except Exception as e:
        duration = time.time() - start_time
        logging.error(f"{script_name} error: {e}")
        print(f"üí• {script_name} - Error: {e}")
        return UpdateResult(script_name, False, str(e), duration, False)

# New: retry wrapper for robustness
def run_update_script_with_retry(script_path: Path, timeout: int = 300, retries: int = 0) -> UpdateResult:
    attempt = 0
    last_result: UpdateResult = None  # type: ignore
    while attempt <= retries:
        result = run_update_script(script_path, timeout)
        if result.success:
            return result
        last_result = result
        attempt += 1
        if attempt <= retries:
            backoff = min(30, 2 ** attempt)
            print(f"üîÅ Retrying {script_path.name} in {backoff}s (attempt {attempt}/{retries})")
            time.sleep(backoff)
    return last_result

def run_sequential(scripts: List[Path], timeout: int, delay: float = 0.0, retries: int = 0, *, fail_fast: bool = False, max_fail: int = 0) -> List[UpdateResult]:
    """Run update scripts sequentially.

    Args:
        scripts: List of script paths to run
        timeout: Timeout per script in seconds
        delay: Optional delay (in seconds) between scripts to avoid overwhelming APIs
        retries: Number of retry attempts per script (default: 0)
    """
    results = []

    failures = 0
    for script_path in scripts:
        result = run_update_script_with_retry(script_path, timeout, retries)
        results.append(result)
        if not result.success:
            failures += 1
            if fail_fast or (max_fail and failures >= max_fail):
                print("‚õî Stopping sequential execution due to failures")
                break

        # Optional delay between scripts
        if delay and delay > 0:
            time.sleep(delay)

    return results

def run_parallel(scripts: List[Path], timeout: int, max_workers: int, *, github_workers: int = 3, microsoft_workers: int = 3, google_workers: int = 4, retries: int = 0, circuit_threshold: int = 3, circuit_sleep: float = 5.0) -> List[UpdateResult]:
    """Run update scripts in parallel with provider-aware throttling."""
    results = []

    # Optional provider mapping from JSON file
    provider_map: Dict[str, str] = {}
    try:
        map_path = SCRIPTS_DIR / 'providers.json'
        if map_path.exists():
            with open(map_path, 'r', encoding='utf-8') as f:
                import json as _json
                provider_map = _json.load(f)
    except Exception:
        provider_map = {}

    def classify_provider(p: Path) -> str:
        name = p.name
        pkg = name.replace('update-', '').replace('.py', '')
        mapped = provider_map.get(name) or provider_map.get(pkg)
        if isinstance(mapped, str) and mapped:
            return mapped
        try:
            with open(p, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read(4000)
                if ('github.com' in content) or ('api.github.com' in content):
                    return 'github'
                if ('learn.microsoft.com' in content) or ('go.microsoft.com' in content) or ('download.microsoft.com' in content) or ('visualstudio.microsoft.com' in content):
                    return 'microsoft'
                if ('googleapis.com' in content) or ('storage.googleapis.com' in content) or ('dl.google.com' in content) or ('cloudfront.net' in content):
                    return 'google'
                return 'other'
        except Exception:
            return 'other'

    # Cap workers to the number of scripts to avoid oversubscription
    max_workers = max(1, min(max_workers, len(scripts)))

    # Compute provider classification and throttling semaphores
    prov_map: Dict[Path, str] = {p: classify_provider(p) for p in scripts}
    counts = {
        'github': sum(1 for v in prov_map.values() if v == 'github'),
        'microsoft': sum(1 for v in prov_map.values() if v == 'microsoft'),
        'google': sum(1 for v in prov_map.values() if v == 'google'),
        'other': sum(1 for v in prov_map.values() if v == 'other'),
    }
    sems = {
        'github': threading.BoundedSemaphore(value=max(1, min(github_workers, max_workers))),
        'microsoft': threading.BoundedSemaphore(value=max(1, min(microsoft_workers, max_workers))),
        'google': threading.BoundedSemaphore(value=max(1, min(google_workers, max_workers))),
        'other': threading.BoundedSemaphore(value=max(1, max_workers)),
    }
    print(f"üîó Provider-aware throttling: GitHub={counts['github']} (max {github_workers}), Microsoft={counts['microsoft']} (max {microsoft_workers}), Google={counts['google']} (max {google_workers}), Other={counts['other']}")

    prov_paused_until: Dict[str, float] = {k: 0.0 for k in ['github', 'microsoft', 'google', 'other']}
    prov_failures: Dict[str, int] = {k: 0 for k in ['github', 'microsoft', 'google', 'other']}
    lock = threading.Lock()

    def _task(script_path: Path, timeout: int) -> UpdateResult:
        prov = prov_map.get(script_path, 'other')
        now = time.time()
        until = prov_paused_until.get(prov, 0.0)
        if until and now < until:
            time.sleep(min(circuit_sleep, until - now))
        with sems.get(prov, sems['other']):
            return run_update_script_with_retry(script_path, timeout, retries)

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all scripts with throttling wrapper
        future_to_script = {
            executor.submit(_task, script_path, timeout): script_path
            for script_path in scripts
        }

        # Collect results as they complete
        for future in concurrent.futures.as_completed(future_to_script):
            script_path = future_to_script[future]
            try:
                result = future.result()
                results.append(result)
                if not result.success:
                    prov = prov_map.get(script_path, 'other')
                    with lock:
                        prov_failures[prov] = prov_failures.get(prov, 0) + 1
                        if circuit_threshold > 0 and prov_failures[prov] >= circuit_threshold:
                            prov_paused_until[prov] = time.time() + circuit_sleep
                            prov_failures[prov] = 0
                            print(f"‚è∏Ô∏è  Pausing {prov} tasks for {circuit_sleep:.1f}s due to failures")
            except Exception as e:
                print(f"üí• {script_path.name} - Unexpected error: {e}")
                results.append(UpdateResult(script_path.name, False, str(e), 0, False))

    # Sort results by script name for consistent output
    results.sort(key=lambda x: x.script_name)
    return results

def print_summary(results: List[UpdateResult], total_duration: float):
    """Print a summary of all update results."""
    print("\n" + "="*80)
    print("üìä UPDATE SUMMARY")
    print("="*80)
    successful = [r for r in results if r.success]
    failed = [r for r in results if not r.success]
    updated = [r for r in results if r.updated]
    print(f"üìà Total Scripts: {len(results)}")
    print(f"‚úÖ Successful: {len(successful)}")
    print(f"‚ùå Failed: {len(failed)}")
    print(f"üîÑ Updated: {len(updated)}")
    print(f"‚è±Ô∏è  Total Duration: {total_duration:.1f}s")
    if updated:
        print(f"\nüéâ PACKAGES UPDATED:")
        for result in updated:
            package_name = result.script_name.replace('update-', '').replace('.py', '')
            version = get_manifest_version(package_name)
            if version:
                print(f"   ‚Ä¢ {package_name}: Update to version {version} ({result.duration:.1f}s)")
            else:
                print(f"   ‚Ä¢ {package_name} ({result.duration:.1f}s)")
    if failed:
        print(f"\n‚ùå FAILED SCRIPTS:")
        for result in failed:
            print(f"   ‚Ä¢ {result.script_name} ({result.duration:.1f}s)")
            error_lines = result.output.strip().split('\n')[:3]
            for line in error_lines:
                if line.strip():
                    print(f"     {line.strip()}")
    no_updates = [r for r in successful if not r.updated]
    if no_updates:
        print(f"\n‚ÑπÔ∏è  NO UPDATES NEEDED:")
        for result in no_updates:
            package_name = result.script_name.replace('update-', '').replace('.py', '')
            version = get_manifest_version(package_name)
            if version:
                print(f"   ‚Ä¢ {package_name} (version {version})")
            else:
                print(f"   ‚Ä¢ {package_name}")
    print("\n" + "="*80)

def write_json_summary(results: List[UpdateResult], total_duration: float, args, mode_label: str) -> None:
    try:
        if not args.json_summary:
            return
        summary_path = Path(args.json_summary)
        summary_path.parent.mkdir(parents=True, exist_ok=True)

        data = {
            "started_at": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "mode": mode_label,
            "timeout": args.timeout,
            "workers": args.workers if args.parallel else 1,
            "provider_workers": {
                "github": args.github_workers,
                "microsoft": args.microsoft_workers,
                "google": args.google_workers,
            },
            "total_duration_seconds": round(total_duration, 3),
            "results": [],
            "counts": {
                "total": len(results),
                "successful": len([r for r in results if r.success]),
                "failed": len([r for r in results if not r.success]),
                "updated": len([r for r in results if r.updated]),
                "no_updates": len([r for r in results if r.success and not r.updated]),
            },
        }

        for r in results:
            pkg = r.script_name.replace('update-', '').replace('.py', '')
            version = get_manifest_version(pkg)
            data["results"].append({
                "script": r.script_name,
                "package": pkg,
                "success": r.success,
                "updated": r.updated,
                "duration_seconds": round(r.duration, 3),
                "version": version or "",
                "error_preview": ("\n".join([ln for ln in r.output.strip().splitlines()[:3]]) if not r.success else ""),
            })

        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"üßæ JSON summary written to: {summary_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to write JSON summary: {e}")

def write_md_summary(results: List[UpdateResult], total_duration: float, args, mode_label: str) -> None:
    try:
        if not getattr(args, 'md_summary', None):
            return
        out_path = Path(args.md_summary)
        out_path.parent.mkdir(parents=True, exist_ok=True)
        counts_total = len(results)
        counts_success = len([r for r in results if r.success])
        counts_failed = len([r for r in results if not r.success])
        counts_updated = len([r for r in results if r.updated])
        lines = []
        lines.append("# Update Health Dashboard")
        lines.append("")
        lines.append(f"- Mode: {mode_label}")
        lines.append(f"- Total: {counts_total}")
        lines.append(f"- Successful: {counts_success}")
        lines.append(f"- Failed: {counts_failed}")
        lines.append(f"- Updated: {counts_updated}")
        lines.append("")
        lines.append("| Package | Version | Success | Updated | Duration (s) |")
        lines.append("|---|---|---|---|---|")
        for r in results:
            pkg = r.script_name.replace('update-', '').replace('.py', '')
            version = get_manifest_version(pkg) or ""
            lines.append(f"| {pkg} | {version} | {str(r.success)} | {str(r.updated)} | {round(r.duration, 3)} |")
        out_path.write_text("\n".join(lines), encoding="utf-8")
        print(f"üßæ Markdown summary written to: {out_path}")
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to write Markdown summary: {e}")

def send_webhook_if_configured(args) -> None:
    try:
        if not args.webhook_url or not args.json_summary:
            return
        payload = None
        try:
            with open(args.json_summary, 'r', encoding='utf-8') as f:
                payload = json.load(f)
        except Exception as e:
            print(f"‚ö†Ô∏è  Webhook skipped: cannot read summary file: {e}")
            return
        headers = {'Content-Type': 'application/json'}
        if args.webhook_header_name and args.webhook_header_value:
            headers[args.webhook_header_name] = args.webhook_header_value
        try:
            from summary_utils import format_webhook_body  # when running from scripts/ as working dir
        except Exception:
            try:
                import importlib.util
                su_path = SCRIPTS_DIR / 'summary_utils.py'
                spec = importlib.util.spec_from_file_location('summary_utils', str(su_path))
                su = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(su)  # type: ignore
                format_webhook_body = su.format_webhook_body
            except Exception:
                def format_webhook_body(payload, webhook_type):
                    return payload
        body = format_webhook_body(payload, getattr(args, 'webhook_type', 'generic'))
        attempts = [0.5, 1.0, 2.0]
        for i, delay in enumerate(attempts):
            try:
                resp = requests.post(args.webhook_url, json=body, headers=headers, timeout=15)
                if 200 <= resp.status_code < 300:
                    print(f"üì£ Webhook delivered: {resp.status_code}")
                    return
                print(f"‚ö†Ô∏è  Webhook failed: {resp.status_code}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Webhook error: {e}")
            time.sleep(delay)
    except Exception as e:
        print(f"‚ö†Ô∏è  Webhook error: {e}")

def filter_resume_paths(script_paths: List[Path], resume_path: Path) -> List[Path]:
    try:
        with open(resume_path, 'r', encoding='utf-8') as f:
            prev = json.load(f)
        failed_scripts = set()
        for item in prev.get('results', []):
            if not bool(item.get('success', False)):
                s = item.get('script')
                if isinstance(s, str) and s:
                    failed_scripts.add(s)
        if failed_scripts:
            return [p for p in script_paths if p.name in failed_scripts]
        return script_paths
    except Exception:
        return script_paths

def check_dependencies():
    """Check if required dependencies are installed."""
    missing_deps = []
    optional_deps = []

    # Check core dependencies
    try:
        import requests
    except ImportError:
        missing_deps.append("requests")

    try:
        import packaging
    except ImportError:
        missing_deps.append("packaging")

    # Check optional dependencies
    try:
        import bs4
    except ImportError:
        optional_deps.append("beautifulsoup4")

    if missing_deps:
        print(f"‚ùå Missing required dependencies: {', '.join(missing_deps)}")
        print("Please install required packages:")
        print(f"pip install {' '.join(missing_deps)}")
        return False

    if optional_deps:
        print(f"‚ö†Ô∏è  Warning: Optional dependencies not found: {', '.join(optional_deps)}")
        print("Some features may be limited. Install with:")
        print(f"pip install {' '.join(optional_deps)}")

    return True

def setup_logging(verbose: bool = False, quiet: bool = False, log_file: Path | None = None) -> None:
    """Configure logging based on verbosity settings."""
    if quiet:
        level = logging.WARNING
    elif verbose:
        level = logging.DEBUG
    else:
        level = logging.INFO

    # Override with environment variable if set
    if LOG_LEVEL != 'INFO':
        level = getattr(logging, LOG_LEVEL, logging.INFO)

    logging.basicConfig(
        level=level,
        format=LOG_FORMAT,
        datefmt=LOG_DATE_FORMAT
    )
    if log_file:
        fh = logging.FileHandler(str(log_file), encoding="utf-8")
        fh.setLevel(level)
        formatter = logging.Formatter(LOG_FORMAT, datefmt=LOG_DATE_FORMAT)
        fh.setFormatter(formatter)
        logging.getLogger().addHandler(fh)

def main():
    """Main function to orchestrate all update scripts."""
    parser = argparse.ArgumentParser(
        description="Run all Scoop bucket update scripts",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python scripts/update-all.py                    # Run all scripts in parallel
  python scripts/update-all.py --sequential       # Run scripts sequentially
  python scripts/update-all.py --workers 8        # Use 8 parallel workers
  python scripts/update-all.py --scripts corecycler esptool  # Run specific scripts
  python scripts/update-all.py --fast --retry 2   # Fast mode with retries
        """
    )

    # Execution mode
    execution_group = parser.add_argument_group('Execution Mode')
    execution_group.add_argument("--parallel", "-p", action="store_true", default=True,
                       help="Run scripts in parallel (default)")
    execution_group.add_argument("--sequential", action="store_true",
                       help="Force sequential execution")
    execution_group.add_argument("--fast", "-f", action="store_true",
                       help="Enable fast mode with optimized worker count")

    # Performance tuning
    performance_group = parser.add_argument_group('Performance')
    performance_group.add_argument("--workers", "-w", type=int, default=DEFAULT_WORKERS,
                       help=f"Number of parallel workers (default: {DEFAULT_WORKERS})")
    performance_group.add_argument("--timeout", "-t", type=int, default=DEFAULT_TIMEOUT,
                       help=f"Timeout per script in seconds (default: {DEFAULT_TIMEOUT})")
    performance_group.add_argument("--delay", "-D", type=float, default=0.0,
                       help="Delay (seconds) between scripts in sequential mode")

    # Provider-specific throttling
    throttling_group = parser.add_argument_group('Provider Throttling')
    throttling_group.add_argument("--github-workers", type=int, default=MAX_GITHUB_WORKERS,
                       help=f"Max concurrent GitHub-related scripts (default: {MAX_GITHUB_WORKERS})")
    throttling_group.add_argument("--microsoft-workers", type=int, default=MAX_MICROSOFT_WORKERS,
                       help=f"Max concurrent Microsoft-related scripts (default: {MAX_MICROSOFT_WORKERS})")
    throttling_group.add_argument("--google-workers", type=int, default=MAX_GOOGLE_WORKERS,
                       help=f"Max concurrent Google-related scripts (default: {MAX_GOOGLE_WORKERS})")
    parser.add_argument("--scripts", "-s", nargs="+",
                       help="Run only specific scripts (e.g., corecycler esptool)")
    parser.add_argument("--skip-scripts", nargs="+",
                       help="Skip specific scripts (e.g., corecycler esptool)")
    parser.add_argument("--only-providers", nargs="+", choices=["github", "microsoft", "google", "other"],
                       help="Run only scripts classified to these providers")
    parser.add_argument("--skip-providers", nargs="+", choices=["github", "microsoft", "google", "other"],
                       help="Skip scripts classified to these providers")
    parser.add_argument("--dry-run", "-d", action="store_true",
                       help="Show what would be run without executing")
    parser.add_argument("--skip-git", action="store_true",
                       help="Skip git add/commit/push after updates")
    parser.add_argument("--git-per-package", action="store_true",
                       help="Stage & commit each updated manifest individually with its own message")
    parser.add_argument("--git-aggregate", action="store_true",
                       help="Stage & commit all changes in aggregate groups (overrides per-package default)")
    parser.add_argument("--structured-output", action="store_true",
                       help="Prefer structured JSON output from update scripts (falls back to text heuristics)")
    parser.add_argument("--http-cache", action="store_true",
                       help="Enable short-lived HTTP response caching for update scripts during this run")
    parser.add_argument("--http-cache-ttl", type=int, default=1800,
                       help="HTTP cache TTL in seconds when --http-cache is enabled (default: 1800)")
    parser.add_argument("--retry", type=int, default=0,
                       help="Number of retry attempts per script on failure (default: 0)")
    parser.add_argument("--json-summary", type=Path,
                       help="Write machine-readable JSON summary to the given file path")
    parser.add_argument("--md-summary", type=Path,
                       help="Write human-friendly Markdown summary to the given file path")
    parser.add_argument("--webhook-url", type=str,
                       help="POST the JSON summary to the given webhook URL")
    parser.add_argument("--webhook-type", type=str, choices=["generic", "slack", "discord"], default="generic",
                       help="Webhook payload format: generic (JSON), slack (text), or discord (content)")
    parser.add_argument("--webhook-header-name", type=str,
                       help="Optional single HTTP header name for webhook request")
    parser.add_argument("--webhook-header-value", type=str,
                       help="Optional single HTTP header value for webhook request")
    parser.add_argument("--fail-fast", action="store_true",
                       help="Stop sequential execution on first failure")
    parser.add_argument("--max-fail", type=int, default=0,
                       help="Stop sequential execution after N failures (0 = no limit)")
    parser.add_argument("--circuit-threshold", type=int, default=3,
                       help="Trigger provider pause after N failures (parallel mode)")
    parser.add_argument("--circuit-sleep", type=float, default=5.0,
                       help="Provider pause duration in seconds (parallel mode)")
    parser.add_argument("--resume", type=Path,
                       help="Resume by rerunning only failed scripts from a previous JSON summary file")
    parser.add_argument("--no-error-exit", action="store_true",
                       help="Always exit with code 0 even if failures occurred")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="Enable verbose logging")
    parser.add_argument("--quiet", "-q", action="store_true",
                       help="Reduce logging output")
    parser.add_argument("--log-file", type=Path,
                       help="Write run log to the given file path")

    args = parser.parse_args()
    if not getattr(args, 'json_summary', None) and os.environ.get('AUTOMATION_JSON_SUMMARY'):
        args.json_summary = Path(os.environ['AUTOMATION_JSON_SUMMARY'])
    if not getattr(args, 'md_summary', None) and os.environ.get('AUTOMATION_MD_SUMMARY'):
        args.md_summary = Path(os.environ['AUTOMATION_MD_SUMMARY'])
    if not getattr(args, 'log_file', None) and os.environ.get('AUTOMATION_LOG_FILE'):
        args.log_file = Path(os.environ['AUTOMATION_LOG_FILE'])

    # Configure logging
    setup_logging(args.verbose, args.quiet, args.log_file)

    # Set structured output preference for parsers
    global PREFER_STRUCTURED_OUTPUT
    PREFER_STRUCTURED_OUTPUT = bool(args.structured_output)

    # Check dependencies
    if not check_dependencies():
        sys.exit(1)

    # Discover available update scripts
    available_scripts = discover_update_scripts()

    # Determine which scripts to run
    if args.scripts:
        # Validate provided script names
        available_scripts_set = set(available_scripts)
        selected_scripts = []

        for script in args.scripts:
            if not script.startswith('update-') or not script.endswith('.py'):
                script = f'update-{script}.py'

            if script in available_scripts_set:
                selected_scripts.append(script)
            else:
                print(f"‚ùå Unknown script: {script}")
                print(f"Available scripts: {', '.join(sorted(available_scripts_set))}")
                sys.exit(1)

        scripts_to_run = selected_scripts
    else:
        scripts_to_run = available_scripts

    if args.skip_scripts:
        normalized_skips = []
        for s in args.skip_scripts:
            if not s.startswith('update-') or not s.endswith('.py'):
                s = f'update-{s}.py'
            normalized_skips.append(s)
        scripts_to_run = [s for s in scripts_to_run if s not in set(normalized_skips)]

    # Verify all script files exist
    script_paths = []
    for script_name in scripts_to_run:
        script_path = SCRIPTS_DIR / script_name
        if script_path.exists():
            script_paths.append(script_path)
        else:
            print(f"‚ö†Ô∏è  Script not found: {script_path}")

    if args.resume:
        before = len(script_paths)
        script_paths = filter_resume_paths(script_paths, Path(args.resume))
        after = len(script_paths)
        if after < before:
            print(f"üîÅ Resuming: {after} failed script(s) will be rerun")
        else:
            print("‚ÑπÔ∏è  Resume requested but no failed scripts found; running full selection")

    # Provider-based filtering before execution
    def load_provider_map() -> Dict[str, str]:
        try:
            p = SCRIPTS_DIR / 'providers.json'
            if p.exists():
                with open(p, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception:
            pass
        return {}

    provider_map = load_provider_map()

    def classify_provider_for_path(p: Path) -> str:
        name = p.name
        pkg = name.replace('update-', '').replace('.py', '')
        mapped = provider_map.get(name) or provider_map.get(pkg)
        if isinstance(mapped, str) and mapped:
            return mapped
        try:
            with open(p, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read(4000)
                if ('github.com' in content) or ('api.github.com' in content):
                    return 'github'
                if ('learn.microsoft.com' in content) or ('go.microsoft.com' in content) or ('download.microsoft.com' in content) or ('visualstudio.microsoft.com' in content):
                    return 'microsoft'
                if ('googleapis.com' in content) or ('storage.googleapis.com' in content) or ('dl.google.com' in content) or ('cloudfront.net' in content):
                    return 'google'
                return 'other'
        except Exception:
            return 'other'

    if args.only_providers:
        allowed = set(args.only_providers)
        script_paths = [p for p in script_paths if classify_provider_for_path(p) in allowed]
        print(f"üéõÔ∏è Provider include filter active: {', '.join(allowed)}")
    if args.skip_providers:
        skipped = set(args.skip_providers)
        before = len(script_paths)
        script_paths = [p for p in script_paths if classify_provider_for_path(p) not in skipped]
        print(f"üö´ Provider skip filter active: {', '.join(skipped)} (removed {before - len(script_paths)})")

    if not script_paths:
        print("‚ùå No valid script files found")
        sys.exit(1)

    # Show what will be run
    logging.info("Starting Scoop Bucket Update Orchestrator")
    print("üîß Scoop Bucket Update Orchestrator")
    print(f"üìÖ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÇ Scripts directory: {SCRIPTS_DIR}")

    # Resolve mode: if --sequential is set, override parallel
    if args.sequential:
        args.parallel = False
        logging.info("Running in sequential mode")
    else:
        args.parallel = True
        logging.info("Running in parallel mode")

    print(f"üéØ Mode: {'Parallel' if args.parallel else 'Sequential'}")
    if args.parallel:
        # Clamp workers to number of scripts
        args.workers = max(1, min(args.workers, len(script_paths)))
        print(f"üë• Workers: {args.workers}")
        logging.info(f"Using {args.workers} parallel workers")
    else:
        print(f"‚è≥ Sequential delay: {args.delay:.1f}s")
        if args.delay > 0:
            logging.info(f"Sequential delay set to {args.delay}s")

    print(f"‚è±Ô∏è  Timeout: {args.timeout}s per script")
    print(f"üìã Scripts to run ({len(script_paths)}):")

    for script_path in script_paths:
        package_name = script_path.name.replace('update-', '').replace('.py', '')
        print(f"   ‚Ä¢ {package_name}")
        logging.debug(f"Will run script for package: {package_name}")

    if args.dry_run:
        print("\nüîç DRY RUN - No scripts will be executed")
        return

    print("\n" + "="*80)

    # Run the scripts
    start_time = time.time()

    # Propagate HTTP cache settings to child processes
    if args.http_cache:
        os.environ['AUTOMATION_HTTP_CACHE'] = '1'
        os.environ['AUTOMATION_HTTP_CACHE_TTL'] = str(args.http_cache_ttl)
    if args.structured_output:
        os.environ['STRUCTURED_ONLY'] = '1'

    # If fast mode is enabled, force parallel with an optimized worker count
    if args.fast:
        args.parallel = True
        # Recommend worker count based on CPU and script count (network-bound tasks benefit from moderate concurrency)
        recommended_workers = min(6, max(3, (os.cpu_count() or 4)))
        args.workers = min(recommended_workers, len(script_paths))
        print(f"‚ö° Fast mode enabled: workers set to {args.workers}")

    if args.parallel:
        results = run_parallel(script_paths, args.timeout, args.workers, github_workers=args.github_workers, microsoft_workers=args.microsoft_workers, google_workers=args.google_workers, retries=args.retry, circuit_threshold=args.circuit_threshold, circuit_sleep=args.circuit_sleep)
    else:
        results = run_sequential(script_paths, args.timeout, args.delay, retries=args.retry, fail_fast=bool(args.fail_fast), max_fail=int(args.max_fail or 0))

    total_duration = time.time() - start_time

    # Print summary
    print_summary(results, total_duration)
    write_json_summary(results, total_duration, args, 'Parallel' if args.parallel else 'Sequential')
    write_md_summary(results, total_duration, args, 'Parallel' if args.parallel else 'Sequential')
    # Optional webhook delivery
    send_webhook_if_configured(args)

    # Exit with appropriate code
    failed_count = len([r for r in results if not r.success])
    if failed_count > 0 and not args.no_error_exit:
        print(f"\n‚ö†Ô∏è  {failed_count} script(s) failed")
        sys.exit(1)
    else:
        # Optionally perform git add/commit/push
        if not args.skip_git:
            print("\n" + "-"*80)
            print("üß© Git integration: staging and committing changes...")
            try:
                use_per_package = args.git_per_package or not args.git_aggregate
                if use_per_package:
                    # Per-package staging/commit using the results
                    updated_results = [r for r in results if r.updated]
                    if not updated_results:
                        print("‚ÑπÔ∏è  No updated packages to commit.")
                    else:
                        stage_and_commit_per_package(updated_results)
                        # Also handle newly added manifests (e.g., from manifest-generator)
                        new_manifests = list_untracked_manifests()
                        for app_name, path in new_manifests:
                            rc, out, err = run_git_command(["git", "add", str(path)])
                            if rc != 0:
                                print(f"‚ö†Ô∏è  git add {path} failed: {err or out}")
                                continue
                            version_str = get_manifest_version(app_name)
                            msg = f"{app_name}: Add version {version_str}" if version_str else f"{app_name}: Add manifest"
                            commit_with_message(msg)
                        push_changes()
                else:
                    # Aggregate commit: stage entire bucket and commit added/updated groups
                    stage_bucket_changes()
                    added_apps, updated_apps = get_staged_bucket_changes()

                    if not added_apps and not updated_apps:
                        print("‚ÑπÔ∏è  No staged changes found under bucket/ to commit.")
                    else:
                        if updated_apps:
                            # Include versions in grouped commit with count
                            updated_with_versions = []
                            for app in updated_apps:
                                v = get_manifest_version(app)
                                updated_with_versions.append(f"{app} {v}" if v else app)
                            msg = f"updated ({len(updated_with_versions)}): " + ", ".join(updated_with_versions)
                            print(f"üìù Committing: {msg}")
                            commit_with_message(msg)
                        if added_apps:
                            added_with_versions = []
                            for app in added_apps:
                                v = get_manifest_version(app)
                                added_with_versions.append(f"{app} {v}" if v else app)
                            msg = f"added ({len(added_with_versions)}): " + ", ".join(added_with_versions)
                            print(f"üìù Committing: {msg}")
                            commit_with_message(msg)
                        push_changes()
            except Exception as e:
                print(f"‚ö†Ô∏è  Git integration encountered an error: {e}")

        print(f"\nüéâ All scripts completed successfully!")
        sys.exit(0)

if __name__ == "__main__":
    main()
